{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57c9fb2-e04c-42e7-b327-3e22e1c128df",
   "metadata": {},
   "source": [
    "# Feature Imputation & Encoding\n",
    "\n",
    "### In this notebook, missing feature data will be imputed using information obtained during the Exploratory Data Analysis to make as many informed and logical imputations as possible. The data will then be encoded and prepared for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61bd3f7-e1d6-4a8a-8b6d-47db57f96e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First start by importing the relevant packages:\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fe832f-4941-4019-9bd5-a478ffb2afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (stored locally)\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab66ac5b-97dd-4db5-85da-10e0e0a02820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again define a function to subdivide the PassengerId and Cabin columns:\n",
    "def column_split(data):\n",
    "    data['FamilyId'] = data.PassengerId.str.extract('(^[0-9]+)')\n",
    "    data[['Cabin_deck','Cabin_number','Cabin_side']] = data.Cabin.str.split('/', expand = True)\n",
    "    data = data.drop(columns = 'Cabin')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1002aa-8472-4438-aba9-e0b27fca449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this function:\n",
    "train = column_split(train)\n",
    "test = column_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca70d6d-ab86-4742-885b-095debfa134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the function to impute the missing data\n",
    "def imputation(data_set):\n",
    "    data = data_set.copy()\n",
    "    # drop the columns that did not appear relevant based on the EDA\n",
    "    data = data.drop(columns = ['Name','VIP','Cabin_number','Cabin_side'])\n",
    "    # now start with imputing Cabin_deck based on FamilyId; most (but technically not all) families will be located near each other,\n",
    "    # so if there are other family members on the passenger list, use their Cabin_deck value\n",
    "    data_Cabin_deck_NA_subset = data.loc[data.Cabin_deck.isna()] # first subset the rows where Cabin_deck is NA\n",
    "    # make a temporary dataframe where Cabin_deck = NA rows are removed\n",
    "    data_Cabin_deck_NA_drop = data.drop(index = data_Cabin_deck_NA_subset.index.tolist())\n",
    "    data_Cabin_deck_NA_subset_Fam_S = data_Cabin_deck_NA_subset.FamilyId # make a series from the FamilyId column of the Cabin_deck = NA rows\n",
    "    for FamId in data_Cabin_deck_NA_subset_Fam_S: # iterate through the FamilyId's...\n",
    "        if sum(data_Cabin_deck_NA_drop.FamilyId.str.contains(FamId)) > 0: # ... and see if there are other family members in the passenger list...\n",
    "            fam_index = data_Cabin_deck_NA_subset_Fam_S.loc[data_Cabin_deck_NA_subset_Fam_S == FamId].index[0]\n",
    "            fam_cabin_deck = data.Cabin_deck.loc[data.FamilyId == FamId].value_counts().index[0]\n",
    "            data.loc[fam_index,'Cabin_deck'] = fam_cabin_deck # ... and if so, set the missing Cabin_deck to the family member Cabin_deck\n",
    "    # Next is ShoppingMall expenditures; if the value of ShoppingMall was >0, then the passenger was likely to be on F deck.\n",
    "    # Use this information to impute some more missing Cabin_deck values\n",
    "    cabin_deck_shopmall_S = data.Cabin_deck.copy()\n",
    "    cabin_deck_shopmall_S = cabin_deck_shopmall_S.mask(cond = (data.Cabin_deck.isnull()) & (data.ShoppingMall > 0),\n",
    "                                                       other = 'F',\n",
    "                                                       axis = 0)\n",
    "    data.Cabin_deck = cabin_deck_shopmall_S\n",
    "    # Some of the Cabin_decks were exclusive to passengers originating from a particular home planet;\n",
    "    # Decks A, B, C, & T only had passengers from Europa\n",
    "    # Deck G only had passengers from Earth\n",
    "    # So passengers located on those decks must have originate from either Europa or Earth, respectively\n",
    "    homeplanet_S = data.HomePlanet.copy()\n",
    "    homeplanet_S = homeplanet_S.mask(cond = (data.Cabin_deck == 'A') | (data.Cabin_deck == 'B') | (data.Cabin_deck == 'C') | (data.Cabin_deck == 'T'),\n",
    "                                     other = 'Europa',\n",
    "                                     axis = 0)\n",
    "    homeplanet_S = homeplanet_S.mask(cond = data.Cabin_deck == 'G', other = 'Earth', axis = 0)\n",
    "    data.HomePlanet = homeplanet_S\n",
    "    # Unable to draw any further inferences on HomePlanet, CryoSleep, or Destination, so replace the remaining NA's in those columns with the mode\n",
    "    data.HomePlanet = data.HomePlanet.replace(to_replace = np.nan, value = 'Earth')\n",
    "    data.CryoSleep = data.CryoSleep.replace(to_replace = np.nan, value = False)\n",
    "    data.Destination = data.Destination.replace(to_replace = np.nan, value = 'TRAPPIST-1e')\n",
    "    # Rather than use the full age distribution to impute age, it is better to use the distribution of the deck for the individual passenger\n",
    "    cabin_decks = data.Cabin_deck.dropna().unique()\n",
    "    for deck in cabin_decks:\n",
    "        cabin_deck_age_S = data.Age.copy()\n",
    "        cabin_deck_age_S = cabin_deck_age_S.mask(cond = (data.Age.isnull()) & (data.Cabin_deck == deck),\n",
    "                                                 other = data.Age.loc[data.Cabin_deck == deck].median(),\n",
    "                                                 axis = 0)\n",
    "        data.Age = cabin_deck_age_S\n",
    "    # Only use full distribution when the location of the passenger is unknown\n",
    "    data.Age = data.Age.replace(to_replace = np.nan, value = data.Age.median())\n",
    "    # Drop FamilyId column\n",
    "    data = data.drop(columns = 'FamilyId')\n",
    "    # Replace any remaining unknown deck locations for passengers with F deck, since that is the most common\n",
    "    data.Cabin_deck = data.Cabin_deck.replace(to_replace = np.nan, value = 'F')\n",
    "    # The mode for expenditures is 0 (and for the most part, so is the median) so replace any unknown expenditures with 0\n",
    "    expenditures = ['RoomService',\n",
    "                    'FoodCourt',\n",
    "                    'ShoppingMall',\n",
    "                    'Spa',\n",
    "                    'VRDeck']\n",
    "    for expenditure in expenditures:\n",
    "        data[expenditure] = data[expenditure].replace(to_replace = np.nan, value = 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07803efb-de06-4125-be10-523bec3a5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now execute for both the training and test data sets\n",
    "train_impute = imputation(train)\n",
    "test_impute = imputation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18632e9-88c3-402a-aa3c-d4b512fed05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now encode the remaining categorical variables using a function\n",
    "def encoding(data_set):\n",
    "    encode_var = ['HomePlanet','Destination','Cabin_deck']\n",
    "    data_encoded = pd.get_dummies(data_set, columns = encode_var, drop_first = True)\n",
    "    return data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3823c255-2462-440e-a4dd-474d5405be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function\n",
    "train_encoded = encoding(train_impute).drop(columns = ['PassengerId'])\n",
    "# remove the PassengerId column from the training set since it is not needed for modeling\n",
    "test_encoded = encoding(test_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e9c6ff-09c9-47ce-b394-2180aeabb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save imputed and encoded features to .csv file in order to import for modeling\n",
    "# train_encoded.to_csv(path_or_buf = 'train_logic_impute.csv', index = False)\n",
    "# test_encoded.to_csv(path_or_buf = 'test_logic_impute.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
